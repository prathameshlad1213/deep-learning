{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network Implementation from Scratch\n",
        "Implement a simple feedforward neural network from scratch in Python without using any in-built deep learning libraries. This implementation focuses on basic components such as forward pass, backward propagation (backpropagation), and training using gradient descent.\n"
      ],
      "metadata": {
        "id": "HSfxMF5qo7qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Definition\n",
        "**Dataset**:\n",
        "The dataset used for this task is a simple binary classification dataset, represented by the \"AND problem.\" The input dataset consists of all possible combinations of binary values (0, 0), (0, 1), (1, 0), (1, 1), with corresponding output labels that represent the AND operation results. The output labels are binary (0 or 1).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Task**:\n",
        "The task is to train a neural network to predict the output of the AND operation. The model should learn the correct classification for each combination of inputs (X = [0, 0], [0, 1], [1, 0], [1, 1]) and produce the corresponding output (Y = [0], [0], [0], [1]).\n"
      ],
      "metadata": {
        "id": "673zvE2Voaf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid Activation Function: Maps any input to a value between 0 and 1.\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Derivative of Sigmoid Activation Function: Used during backpropagation to calculate the gradient.\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "# Neural Network Class Definition\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):      #  Initialize the neural network with the given sizes for the input, hidden, and output layers.\n",
        "\n",
        "        self.input_size = input_size      # Number of input features\n",
        "        self.hidden_size = hidden_size    # Number of neurons in the hidden layer\n",
        "        self.output_size = output_size    # Number of output neurons\n",
        "\n",
        "        # Initialize weights and biases closer to zero\n",
        "        self.weights_input_hidden = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])  # Input to hidden\n",
        "        self.bias_hidden = np.array([[0.1, 0.1, 0.1]])  # Hidden layer bias\n",
        "\n",
        "        self.weights_hidden_output = np.array([[0.2], [0.3], [0.4]])  # Hidden to output\n",
        "        self.bias_output = np.array([[0.1]])  # Output layer bias\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "      # Perform the forward pass of the neural network. Compute the activations for the input, hidden, and output layers.\n",
        "\n",
        "        self.input_layer = X    # Store the input data\n",
        "\n",
        "        # Calculate the input to the hidden layer and apply the activation function\n",
        "        self.hidden_layer_input = np.dot(self.input_layer, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Calculate the input to the output layer and apply the activation function\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        # Perform the backward pass of the neural network (backpropagation). This step adjusts the weights based on the error in the output.\n",
        "\n",
        "        # Compute the error in the output layer\n",
        "        error_output = y - self.output_layer_output\n",
        "\n",
        "        # Calculate the gradient (delta) for the output layer\n",
        "        output_layer_delta = error_output * sigmoid_derivative(self.output_layer_output)\n",
        "\n",
        "        # Compute the error in the hidden layer\n",
        "        error_hidden = output_layer_delta.dot(self.weights_hidden_output.T)\n",
        "\n",
        "        # Calculate the gradient (delta) for the hidden layer\n",
        "        hidden_layer_delta = error_hidden * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        # Update weights from hidden to output layer\n",
        "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the output layer\n",
        "        self.bias_output += np.sum(output_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        # Update weights from input to hidden layer\n",
        "        self.weights_input_hidden += X.T.dot(hidden_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the hidden layer\n",
        "        self.bias_hidden += np.sum(hidden_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        # Train the neural network on the provided data using the forward and backward passes.\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X)   # Perform a forward pass\n",
        "\n",
        "            self.backward(X, y, learning_rate)     # Perform a backward pass (backpropagation)\n",
        "\n",
        "            # Print loss (mean squared error) every 1000 epochs\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - self.output_layer_output))\n",
        "                print(f\"Epoch {epoch} - Loss: {loss}\")\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    # AND Problem Dataset\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "    # Increased hidden layer size to 3 neurons\n",
        "    nn = NeuralNetwork(input_size=2, hidden_size=3, output_size=1)\n",
        "\n",
        "    # Train for 10,000 epochs with a smaller learning rate for better convergence\n",
        "    nn.train(X, y, epochs=10000, learning_rate=0.05)\n",
        "\n",
        "    print(\"\\nPredictions after training:\")\n",
        "    print(nn.forward(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNkbZEllhG2S",
        "outputId": "7d1bcec2-4a23-4e32-e81c-d0982e6e2995"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.3450465038318372\n",
            "Epoch 1000 - Loss: 0.1783496365671266\n",
            "Epoch 2000 - Loss: 0.12429939968170604\n",
            "Epoch 3000 - Loss: 0.047873714906797635\n",
            "Epoch 4000 - Loss: 0.01855949209681791\n",
            "Epoch 5000 - Loss: 0.00952376878153196\n",
            "Epoch 6000 - Loss: 0.0059349349666032985\n",
            "Epoch 7000 - Loss: 0.004155656051742066\n",
            "Epoch 8000 - Loss: 0.0031328199738713818\n",
            "Epoch 9000 - Loss: 0.0024827358480836526\n",
            "\n",
            "Predictions after training:\n",
            "[[0.002112  ]\n",
            " [0.04612534]\n",
            " [0.0463195 ]\n",
            " [0.93772122]]\n"
          ]
        }
      ]
    }
  ]
}